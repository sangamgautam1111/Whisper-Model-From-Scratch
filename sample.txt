"""
Dataset and DataLoader for Whisper Model
Handles loading audio files and their transcriptions
"""

import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import pandas as pd
from typing import Tuple, List, Dict
import config
from data_preprocessing import AudioPreprocessor

class WhisperDataset(Dataset):
    """
    Custom Dataset for Whisper model
    Loads audio files and their corresponding transcriptions
    """
    
    def __init__(self, 
                 audio_paths: List[str], 
                 transcripts: List[str],
                 preprocessor: AudioPreprocessor,
                 tokenizer,
                 is_training: bool = True):
        """
        Initialize the dataset
        
        Args:
            audio_paths: List of paths to audio files
            transcripts: List of corresponding text transcriptions
            preprocessor: AudioPreprocessor instance
            tokenizer: Tokenizer for text processing
            is_training: Whether this is training data (enables augmentation)
        """
        # Store the data
        self.audio_paths = audio_paths
        self.transcripts = transcripts
        self.preprocessor = preprocessor
        self.tokenizer = tokenizer
        self.is_training = is_training
        
        # Validate that we have matching audio and transcripts
        assert len(audio_paths) == len(transcripts), \
            "Number of audio files must match number of transcripts"
        
    def __len__(self) -> int:
        """
        Return the total number of samples in dataset
        """
        return len(self.audio_paths)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        Get a single sample from the dataset
        
        Args:
            idx: Index of the sample to retrieve
            
        Returns:
            sample: Dictionary containing audio features and token IDs
        """
        # Get audio path and transcript for this index
        audio_path = self.audio_paths[idx]
        transcript = self.transcripts[idx]
        
        # Preprocess audio (includes augmentation if training)
        audio_features = self.preprocessor.preprocess(
            audio_path, 
            apply_augmentation=self.is_training
        )
        
        # Tokenize transcript text
        # Add special tokens: <start> + transcript + <end>
        tokens = self.tokenizer.encode(
            transcript,
            add_special_tokens=True,  # Add START and END tokens
            max_length=config.MAX_TEXT_LENGTH,  # Limit length
            truncation=True,  # Truncate if too long
            padding='max_length',  # Pad to max length
            return_tensors='pt'  # Return as PyTorch tensor
        )
        
        # Remove batch dimension (tokenizer adds it)
        tokens = tokens.squeeze(0)
        
        # Create sample dictionary
        sample = {
            'audio_features': audio_features,  # Shape: (1, time, n_mels)
            'tokens': tokens,  # Shape: (max_text_length,)
            'transcript': transcript  # Original text (for reference)
        }
        
        return sample


def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:
    """
    Collate function to batch multiple samples together
    Handles padding and creates proper batch tensors
    
    Args:
        batch: List of samples from __getitem__
        
    Returns:
        batched: Dictionary of batched tensors
    """
    # Extract audio features from all samples
    audio_features = [item['audio_features'] for item in batch]
    # Stack into batch: (batch, 1, time, n_mels)
    audio_features = torch.stack(audio_features, dim=0)
    
    # Extract token sequences from all samples
    tokens = [item['tokens'] for item in batch]
    # Stack into batch: (batch, max_text_length)
    tokens = torch.stack(tokens, dim=0)
    
    # Create attention masks
    # For audio: mask out padding (all zeros are padding)
    audio_mask = (audio_features.sum(dim=-1).sum(dim=-1) == 0).squeeze(1)  # (batch, time)
    
    # For tokens: mask out padding tokens (PAD_TOKEN_ID is padding)
    token_mask = (tokens == config.PAD_TOKEN_ID)  # (batch, max_text_length)
    
    # Extract original transcripts (for debugging/logging)
    transcripts = [item['transcript'] for item in batch]
    
    # Create input tokens (all tokens except last)
    # This is what decoder receives as input
    input_tokens = tokens[:, :-1]
    
    # Create target tokens (all tokens except first)
    # This is what decoder should predict
    target_tokens = tokens[:, 1:]
    
    # Transpose for transformer format: (seq_len, batch)
    input_tokens = input_tokens.transpose(0, 1)
    target_tokens = target_tokens.transpose(0, 1)
    
    # Adjust token mask for decoder input (remove last token mask)
    decoder_mask = token_mask[:, :-1]
    
    return {
        'audio_features': audio_features,  # (batch, 1, time, n_mels)
        'input_tokens': input_tokens,  # (seq_len, batch)
        'target_tokens': target_tokens,  # (seq_len, batch)
        'audio_mask': audio_mask,  # (batch, time)
        'decoder_mask': decoder_mask,  # (batch, seq_len)
        'transcripts': transcripts  # List of original text
    }


def create_dataloaders(train_audio_paths: List[str],
                       train_transcripts: List[str],
                       val_audio_paths: List[str],
                       val_transcripts: List[str],
                       tokenizer,
                       batch_size: int = config.BATCH_SIZE,
                       num_workers: int = config.NUM_WORKERS) -> Tuple[DataLoader, DataLoader]:
    """
    Create train and validation dataloaders
    
    Args:
        train_audio_paths: List of training audio file paths
        train_transcripts: List of training transcriptions
        val_audio_paths: List of validation audio file paths
        val_transcripts: List of validation transcriptions
        tokenizer: Tokenizer for text processing
        batch_size: Batch size for dataloaders
        num_workers: Number of worker processes for data loading
        
    Returns:
        train_loader: DataLoader for training data
        val_loader: DataLoader for validation data
    """
    # Create audio preprocessor
    preprocessor = AudioPreprocessor()
    
    # Create training dataset (with augmentation)
    train_dataset = WhisperDataset(
        audio_paths=train_audio_paths,
        transcripts=train_transcripts,
        preprocessor=preprocessor,
        tokenizer=tokenizer,
        is_training=True  # Enable augmentation
    )
    
    # Create validation dataset (no augmentation)
    val_dataset = WhisperDataset(
        audio_paths=val_audio_paths,
        transcripts=val_transcripts,
        preprocessor=preprocessor,
        tokenizer=tokenizer,
        is_training=False  # Disable augmentation
    )
    
    # Create training dataloader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,  # Shuffle training data
        num_workers=num_workers,  # Parallel data loading
        collate_fn=collate_fn,  # Custom batching function
        pin_memory=True,  # Faster transfer to GPU
        drop_last=True  # Drop incomplete batches
    )
    
    # Create validation dataloader
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,  # Don't shuffle validation data
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=False  # Keep all validation samples
    )
    
    print(f"Created dataloaders:")
    print(f"  Training samples: {len(train_dataset)}")
    print(f"  Validation samples: {len(val_dataset)}")
    print(f"  Batch size: {batch_size}")
    print(f"  Training batches: {len(train_loader)}")
    print(f"  Validation batches: {len(val_loader)}")
    
    return train_loader, val_loader


def load_data_from_csv(csv_path: str) -> Tuple[List[str], List[str]]:
    """
    Load audio paths and transcripts from CSV file
    
    Expected CSV format:
        audio_path,transcript
        /path/to/audio1.wav,hello world
        /path/to/audio2.wav,this is a test
    
    Args:
        csv_path: Path to CSV file
        
    Returns:
        audio_paths: List of audio file paths
        transcripts: List of transcriptions
    """
    # Read CSV file
    df = pd.read_csv(csv_path)
    
    # Extract audio paths and transcripts
    audio_paths = df['audio_path'].tolist()
    transcripts = df['transcript'].tolist()
    
    # Validate that files exist
    valid_indices = []
    for idx, path in enumerate(audio_paths):
        if Path(path).exists():
            valid_indices.append(idx)
        else:
            print(f"Warning: Audio file not found: {path}")
    
    # Keep only valid samples
    audio_paths = [audio_paths[i] for i in valid_indices]
    transcripts = [transcripts[i] for i in valid_indices]
    
    print(f"Loaded {len(audio_paths)} samples from {csv_path}")
    
    return audio_paths, transcripts


# Example usage
if __name__ == "__main__":
    from transformers import WhisperTokenizer
    
    print("=" * 80)
    print("Testing Dataset and DataLoader")
    print("=" * 80)
    
    # Initialize tokenizer
    tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-tiny")
    
    # Example: Create dummy data
    # In practice, you would load this from CSV files
    dummy_audio_paths = [str(config.RAW_DATA_DIR / f"audio{i}.wav") for i in range(10)]
    dummy_transcripts = [f"This is sample transcript number {i}" for i in range(10)]
    
    # Split into train and validation
    split_idx = int(len(dummy_audio_paths) * config.TRAIN_VAL_SPLIT)
    train_paths = dummy_audio_paths[:split_idx]
    train_transcripts = dummy_transcripts[:split_idx]
    val_paths = dummy_audio_paths[split_idx:]
    val_transcripts = dummy_transcripts[split_idx:]
    
    print(f"\nTrain samples: {len(train_paths)}")
    print(f"Val samples: {len(val_paths)}")
    
    # Note: This will fail if audio files don't exist
    # In practice, you would have real audio files
    print("\nTo test with real data:")
    print("1. Place audio files in:", config.RAW_DATA_DIR)
    print("2. Create a CSV with columns: audio_path, transcript")
    print("3. Use load_data_from_csv() to load your data")
    
    print("\n" + "=" * 80)